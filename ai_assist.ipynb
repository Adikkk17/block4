{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb99e6b-02f8-4573-98a0-e469d2595688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Constitution Assistant in Jupyter Notebook\n",
    "# Kazakh, Russian, English support | KazLLM + Ollama + LangChain\n",
    "\n",
    "# üì¶ STEP 1: Imports and Environment\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langdetect import detect\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f5254-7fa9-4c1b-989d-dc93e9b4d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ STEP 2: Language Detection\n",
    "def detect_language(text):\n",
    "    lang_code = detect(text)\n",
    "    return {\n",
    "        'kk': 'kazakh',\n",
    "        'ru': 'russian',\n",
    "        'en': 'english'\n",
    "    }.get(lang_code, 'unknown')\n",
    "\n",
    "# ‚úÖ STEP 3: Load KazLLM from HuggingFace (once)\n",
    "def load_kazllm():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"kaznlp/kazllm-7b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"kaznlp/kazllm-7b\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "    return pipe\n",
    "\n",
    "kazllm_pipe = load_kazllm()\n",
    "\n",
    "# ‚úÖ STEP 4: Set file paths\n",
    "files = {\n",
    "    'kazakh': 'constitution_kz.txt',\n",
    "    'russian': 'constitution_ru.txt',\n",
    "    'english': 'constitution_en.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0437b-1882-4942-ad9b-48d8ddf985c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ STEP 5: Function to load and split documents\n",
    "def get_chunks(file_path):\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "# ‚úÖ STEP 6: Retrieve relevant chunks via Chroma\n",
    "retriever_cache = {}\n",
    "\n",
    "def get_retriever(lang):\n",
    "    if lang not in retriever_cache:\n",
    "        chunks = get_chunks(files[lang])\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        db = Chroma.from_documents(chunks, embedding=embeddings)\n",
    "        retriever_cache[lang] = db.as_retriever()\n",
    "    return retriever_cache[lang]\n",
    "\n",
    "# ‚úÖ STEP 7: Answer a question\n",
    "def get_answer(question):\n",
    "    lang = detect_language(question)\n",
    "    if lang == 'unknown':\n",
    "        return \"‚õî Unsupported language. Please use Kazakh, Russian or English.\"\n",
    "    print(f\"üåê Language detected: {lang}\")\n",
    "    retriever = get_retriever(lang)\n",
    "    context_docs = retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "    if lang == 'kazakh':\n",
    "        prompt = f\"{context_text}\\n–°“±—Ä–∞“õ: {question}\\n–ñ–∞—É–∞–ø:\"\n",
    "        result = kazllm_pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)\n",
    "        return result[0]['generated_text'].split(\"–ñ–∞—É–∞–ø:\")[-1].strip()\n",
    "    else:\n",
    "        llm = ChatOllama(model=\"mistral\")\n",
    "        qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "        return qa.run(question)\n",
    "\n",
    "# ‚úÖ STEP 8: Ask a question\n",
    "example_question = \"“ö–∞–∑–∞“õ—Å—Ç–∞–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∞—Å—ã–Ω—ã“£ –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç—ñ–Ω—ñ“£ ”©–∫—ñ–ª–µ—Ç—Ç—ñ–∫—Ç–µ—Ä—ñ “õ–∞–Ω–¥–∞–π?\"\n",
    "response = get_answer(example_question)\n",
    "print(\"\\nüìå –ñ–∞—É–∞–ø / –û—Ç–≤–µ—Ç / Answer:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e27cb-e03e-4fe3-a017-d906d9ff9eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9653f1a-946c-4229-9eae-ee0e9c2d8594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610adb29-463f-4df1-914e-457ebc86265b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6551cf-9671-45e9-a4c3-332a10c0f268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153adf1a-e4dd-45fe-86c2-f23186cd2209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736db616-a128-48ea-b8a1-5348ed688f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
